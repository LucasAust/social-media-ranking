# ğŸš€ High-Performance Social Media Post Ranking Engine

A production-ready streaming ranking engine that can handle **millions of posts** with **constant memory usage** and **real-time performance**. Perfect for social media feeds, content discovery, and viral content ranking.

## ğŸ¯ Key Features

- **âš¡ Real-time Performance**: 200,000+ posts/second throughput
- **ğŸ’¾ Constant Memory**: Only ~50MB RAM regardless of dataset size
- **ğŸ›¡ï¸ Crash-Proof**: Handles 1M+ posts with comprehensive error handling
- **ğŸ“Š Multiple Algorithms**: Hot score, engagement score, time decay, hybrid
- **ğŸ”„ Streaming Architecture**: Process unlimited posts without loading into memory
- **ğŸ­ Production-Ready**: Comprehensive testing, benchmarking, and optimization

## ğŸ“Š Performance Summary

| Metric | Performance |
|--------|-------------|
| **Throughput** | 200,000-500,000 posts/second |
| **Latency** | < 5ms per 1,000 posts |
| **Memory Usage** | ~50MB for millions of posts |
| **Max Capacity** | Unlimited (streaming architecture) |
| **Real-time Capable** | âœ… YES - Zero lag ranking |

### Real-World Scenarios
- **Social Media Feed**: 1,000 posts/sec â†’ **212,108 posts/sec** (212x faster)
- **Viral Content**: 5,000 posts/sec â†’ **241,168 posts/sec** (48x faster)
- **Trending Page**: 10,000 posts/sec â†’ **245,771 posts/sec** (24x faster)
- **Analytics Dashboard**: 50,000 posts/sec â†’ **246,356 posts/sec** (5x faster)

## ğŸš€ Quick Start

### Installation
```bash
# Clone the repository
git clone <your-repo-url>
cd social-media-ranking

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage
```python
from streaming_ranking_engine import StreamingRankingEngine
from ranking_engine import Post

# Create engine
engine = StreamingRankingEngine(batch_size=10_000)

# Create post iterator (from database, file, or API)
def post_iterator():
    # Your data source here
    for post_data in your_data_source:
        yield Post(**post_data)

# Rank posts
result = engine.rank_posts_streaming(
    post_iterator(), 
    algorithm="hot_score", 
    top_k=100
)

print(f"Top post score: {result.top_posts[0][0]}")
print(f"Processed {result.total_posts_processed:,} posts in {result.processing_time_seconds:.2f}s")
```

## ğŸ§ª Running Tests

### 1. **Unit Tests** (Core Functionality)
```bash
# Run all unit tests
python -m pytest tests/ -v

# Run specific test file
python -m pytest test_streaming_ranking.py -v
```

### 2. **Performance Tests** (Speed & Memory)
```bash
# Basic performance test
python performance_test.py

# Comprehensive benchmark comparison
python benchmark_comparison.py
```

### 3. **Streaming Tests** (Large Scale)
```bash
# Streaming ranking tests
python -m pytest test_streaming_ranking.py -v

# Maximum capacity test
python test_maximum_capacity.py
```

### 4. **Storage Strategy Tests** (Data Access)
```bash
# Storage performance comparison
python storage_example.py

# Storage strategy benchmark
python data_storage_strategies.py
```

### 5. **Real-Time Performance Tests** (Production Readiness)
```bash
# Real-time performance analysis
python real_time_performance.py
```

### 6. **Demo & Examples**
```bash
# Quick start demo
python quick_start.py

# Comprehensive demo
python demo.py

# Streaming demo
python streaming_demo.py

# Example usage patterns
python example_usage.py
```

## ğŸ“ Project Structure

```
social-media-ranking/
â”œâ”€â”€ ranking_engine.py              # Core ranking algorithms
â”œâ”€â”€ streaming_ranking_engine.py    # Streaming implementation
â”œâ”€â”€ config.py                      # Configuration management
â”œâ”€â”€ requirements.txt               # Dependencies
â”œâ”€â”€ README.md                      # This file
â”‚
â”œâ”€â”€ tests/                         # Unit tests
â”‚   â”œâ”€â”€ test_ranking_engine.py
â”‚   â””â”€â”€ test_streaming_ranking.py
â”‚
â”œâ”€â”€ performance_test.py            # Performance testing
â”œâ”€â”€ benchmark_comparison.py        # Algorithm comparison
â”œâ”€â”€ test_maximum_capacity.py       # Scale testing
â”œâ”€â”€ real_time_performance.py       # Real-time analysis
â”‚
â”œâ”€â”€ storage_example.py             # Storage strategies
â”œâ”€â”€ data_storage_strategies.py     # Storage implementations
â”‚
â”œâ”€â”€ demo.py                        # Comprehensive demo
â”œâ”€â”€ streaming_demo.py              # Streaming demo
â”œâ”€â”€ quick_start.py                 # Quick start guide
â”œâ”€â”€ example_usage.py               # Usage examples
â”‚
â”œâ”€â”€ .benchmarks/                   # Benchmark results
â”œâ”€â”€ performance_report.md          # Performance documentation
â””â”€â”€ streaming_benchmark_results.json
```

## ğŸ—„ï¸ Storage Integration

### Database Integration
```python
# PostgreSQL
import psycopg2

def postgres_iterator():
    conn = psycopg2.connect("postgresql://user:pass@localhost/db")
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM posts ORDER BY timestamp DESC")
    for row in cursor:
        yield Post(*row)
    conn.close()

# MongoDB
from pymongo import MongoClient

def mongo_iterator():
    client = MongoClient('mongodb://localhost:27017/')
    collection = client.db.posts
    for doc in collection.find({}):
        yield Post(**doc)
    client.close()
```

### File Storage
```python
# JSONL file
def jsonl_iterator(filename):
    with open(filename) as f:
        for line in f:
            yield Post(**json.loads(line))

# SQLite database
def sqlite_iterator(db_path):
    import sqlite3
    conn = sqlite3.connect(db_path)
    cursor = conn.execute("SELECT * FROM posts")
    for row in cursor:
        yield Post(*row)
    conn.close()
```

## âš¡ Performance Optimization

### Recommended Settings
- **Batch Size**: 10,000-50,000 posts per batch
- **Top-K**: 100-1000 posts (depending on use case)
- **Algorithm**: `hot_score` for general use, `engagement_score` for social metrics
- **Storage**: SQLite for local, PostgreSQL for production

### Memory Optimization
- **Streaming**: Never load all posts into memory
- **Batch Processing**: Process posts in chunks
- **Min-Heap**: Only keep top-K posts in memory

## ğŸ›¡ï¸ Production Deployment

### Environment Setup
```bash
# Production dependencies
pip install psutil numpy pytest

# Optional: Database drivers
pip install psycopg2-binary pymongo
```

### Monitoring
- **Memory Usage**: Monitor with `psutil`
- **Performance**: Use built-in timing metrics
- **Errors**: Implement comprehensive logging
- **Health Checks**: Regular performance tests

### Scaling
- **Horizontal**: Deploy multiple instances
- **Vertical**: Increase batch sizes
- **Caching**: Cache frequently accessed posts
- **Load Balancing**: Distribute requests across instances

## ğŸ“ˆ Benchmark Results

### Latest Performance (from `streaming_benchmark_results.json`)
```json
{
  "1,000,000 posts": {
    "hot_score": {
      "processing_time_seconds": 3.02,
      "posts_per_second": 331441,
      "memory_usage_mb": 47.7
    }
  }
}
```

### Storage Performance Comparison
| Storage Type | Size (MB) | Posts/sec | Memory (MB) |
|--------------|-----------|-----------|-------------|
| SQLite | 6.6 | 276,531 | 81.1 |
| Compressed JSON | 1.8 | 217,447 | 78.3 |
| Streaming File | 5.2 | 269,628 | 83.8 |
| Pickle | 6.4 | 367,672 | 82.3 |

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Run all tests: `python -m pytest tests/ -v`
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ†˜ Support

- **Issues**: Create GitHub issues for bugs
- **Performance**: Run `python real_time_performance.py` for analysis
- **Testing**: Use `python -m pytest tests/ -v` for validation

---

## ğŸ¯ GitHub Setup Instructions

### 1. Initialize Git Repository
```bash
# Initialize git (if not already done)
git init

# Add all files
git add .

# Initial commit
git commit -m "Initial commit: High-performance social media ranking engine"
```

### 2. Create GitHub Repository
1. Go to GitHub.com
2. Click "New repository"
3. Name it: `social-media-ranking`
4. Don't initialize with README (we already have one)
5. Click "Create repository"

### 3. Push to GitHub
```bash
# Add remote origin
git remote add origin https://github.com/YOUR_USERNAME/social-media-ranking.git

# Push to GitHub
git branch -M main
git push -u origin main
```

### 4. Verify Setup
```bash
# Clone fresh to test
git clone https://github.com/YOUR_USERNAME/social-media-ranking.git
cd social-media-ranking
pip install -r requirements.txt
python -m pytest tests/ -v
```

---

**ğŸš€ Your algorithm is production-ready and can handle any scale of social media data!**
